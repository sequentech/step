# SPDX-FileCopyrightText: 2023-2024 Eduardo Robles <edu@sequentech.io>
#
# SPDX-License-Identifier: AGPL-3.0-only

services:
  devcontainer:
    profiles: ["full", "base"]
    image: ghcr.io/cachix/devenv/devcontainer:v1.10
    container_name: devcontainer
    volumes:
    - type: bind
      source: ../..
      target: /workspaces
      bind:
        create_host_path: true
        selinux: z
    # support ptrace-based debuggers like C++, Go, and Rust
    cap_add:
     - SYS_PTRACE
    security_opt:
      - label:disable
    environment:
      # postgres database to store Hasura metadata
      HASURA_GRAPHQL_METADATA_DATABASE_URL: ${HASURA_GRAPHQL_METADATA_DATABASE_URL}
      # this env var can be used to add the above postgres database to Hasura
      # as a data source. this can be removed/updated based on your needs
      PG_DATABASE_URL: ${HASURA_PG_DATABASE_URL}

  simplesaml:
    profiles: ["full"]
    image: kenchan0130/simplesamlphp
    container_name: simplesamlphp
    ports:
      - "8083:8080"
    volumes:
      - ./simplesaml/authsources.php:/var/www/simplesamlphp/config/authsources.php
      - ./simplesaml/saml20-sp-remote.php:/var/www/simplesamlphp/metadata/saml20-sp-remote.php

  # Needed to set up proper permissions for non root user in postgres
  postgres-volume-init:
    profiles: ["full", "base"]
    image: alpine
    container_name: postgres-volume-init
    volumes:
      - db_logs:/logs
      - keycloak_db_logs:/keycloak_logs
      - b3_db_logs:/b3_logs
    user: root
    group_add:
      - '999'
    command: chown -R 999:999 /logs /keycloak_logs /b3_logs

  postgres:
    profiles: ["full", "base"]
    # Required by VACUUM ANALYZE to work properly
    shm_size: 256m
    image: sequentech.local/postgresql
    container_name: postgres
    build:
      context: ./postgresql
      dockerfile: Dockerfile
    restart: unless-stopped
    volumes:
      - db_data:/var/lib/postgresql
      - db_logs:/logs
    environment:
      POSTGRES_PASSWORD: ${HASURA_PG_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 10s
      retries: 15
      start_period: 5s
    command: postgres -c 'config_file=/etc/postgresql/postgresql.conf'
    depends_on:
      - postgres-volume-init

  minio:
    profiles: ["full", "base"]
    container_name: minio
    image: minio/minio
    #recommend way for docker-outside-of-docker is using devcontainer.json forwardPorts
    #More info: https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-from-docker-compose/.devcontainer/docker-compose.yml#L28
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_storage:/data
    environment:
      - MINIO_ROOT_USER=${AWS_S3_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${AWS_S3_ROOT_PASSWORD}
      - MINIO_ACCESS_KEY=${AWS_S3_ACCESS_KEY}
      - MINIO_ACCESS_SECRET=${AWS_S3_ACCESS_SECRET}
    #entrypoint: sh
    #command: -c 'mc config host add minio http://localhost:9000 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD && mc admin user svcacct add --access-key "$MINIO_ACCESS_KEY" --secret-key "$MINIO_ACCESS_SECRET" minio $MINIO_ROOT_USER && server --console-address ":9001" /data'
    command: server --console-address ":9001" /data
    #entrypoint: /bin/bash
    #command: -c 'minio server --console-address ":9001" /data'
    #restart: always
    #depends_on:
    #  - graphql-engine
    healthcheck:
      test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/live"]
      interval: 5s
      timeout: 10s
      retries: 25
      start_period: 5s

  # used to add headers to minio
  minio-proxy:
    profiles: ["full", "base"]
    container_name: minio-proxy
    image: nginx:latest
    ports:
      - "9002:9002"
    volumes:
      - ./minio/nginx:/etc/nginx/conf.d
    #image: minio/mc
    depends_on:
      - minio

  configure-minio:
    profiles: ["full", "base"]
    container_name: configure-minio
    build:
      context: ./minio
      dockerfile: Dockerfile
    depends_on:
      minio:
        condition: service_healthy
    environment:
      - MINIO_ROOT_USER=${AWS_S3_ROOT_USER}
      - MINIO_ROOT_PASSWORD=${AWS_S3_ROOT_PASSWORD}
      - MINIO_ACCESS_KEY=${AWS_S3_ACCESS_KEY}
      - MINIO_ACCESS_SECRET=${AWS_S3_ACCESS_SECRET}
      - MINIO_PRIVATE_URI=${AWS_S3_PRIVATE_URI}
      - MINIO_PUBLIC_BUCKET=${AWS_S3_PUBLIC_BUCKET}
      - MINIO_BUCKET=${AWS_S3_BUCKET}
    entrypoint: /scripts/entrypoint.sh

  # hashicorp vault to store secrets
  # vault:
  #   profiles: ["full", "base"]
  #   container_name: vault
  #   build:
  #     context: ./vault
  #     dockerfile: Dockerfile
  #   restart: on-failure:10
  #   #recommend way for docker-outside-of-docker is using devcontainer.json forwardPorts
  #   #More info: https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-from-docker-compose/.devcontainer/docker-compose.yml#L28
  #   ports:
  #     - "8201:8201"
  #     - "8200:8200"
  #   environment:
  #     VAULT_API_ADDR: 'http://0.0.0.0:8200'
  #     VAULT_ADDR: 'http://0.0.0.0:8201'
  #     VAULT_UNSEAL_KEY: ciWE5G/CT7/uo5mfaGeRvSyuGRnbtijzvLDg3ru/jv0=
  #     VAULT_TOKEN: hvs.s0djsk0LLBI19K0DkW4Fajs7
  #   cap_add:
  #     - IPC_LOCK
  #   volumes:
  #     - vault-volume:/vault
  #   healthcheck:
  #     retries: 5
  #   entrypoint: /opt/vault/entrypoint.sh

  graphql-engine:
    profiles: ["full", "base"]
    image: hasura/graphql-engine:v2.33.1.cli-migrations-v3
    container_name: hasura
    ports:
      - "8080:8080"
    restart: always
    volumes_from:
    - devcontainer
    environment:
      # applies migrations on start
      HASURA_GRAPHQL_MIGRATIONS_SERVER_TIMEOUT: 60
      HASURA_GRAPHQL_MIGRATIONS_DIR: /workspaces/step/hasura/migrations
      HASURA_GRAPHQL_METADATA_DIR: /workspaces/step/hasura/metadata
      ## postgres database to store Hasura metadata
      HASURA_GRAPHQL_METADATA_DATABASE_URL: ${HASURA_GRAPHQL_METADATA_DATABASE_URL}
      ## this env var can be used to add the above postgres database to Hasura as a data source. this can be removed/updated based on your needs
      PG_DATABASE_URL: ${HASURA_PG_DATABASE_URL}
      ## enable the console served by server
      # set to "false" to disable console
      HASURA_GRAPHQL_ENABLE_CONSOLE: ${HASURA_GRAPHQL_ENABLE_CONSOLE}
      ## enable debugging mode. It is recommended to disable this in production
      HASURA_GRAPHQL_DEV_MODE: ${HASURA_GRAPHQL_DEV_MODE}
      # https://hasura.io/docs/latest/deployment/graphql-engine-flags/config-examples/#console-assets-on-server
      ## uncomment next line to run console offline (i.e load console assets from server instead of CDN)
      HASURA_GRAPHQL_CONSOLE_ASSETS_DIR: ${HASURA_GRAPHQL_CONSOLE_ASSETS_DIR}
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: ${HASURA_GRAPHQL_ENABLED_LOG_TYPES}
      HASURA_GRAPHQL_METADATA_DEFAULTS: ${HASURA_GRAPHQL_METADATA_DEFAULTS}
      # Hasura role for unauthorized users
      HASURA_GRAPHQL_UNAUTHORIZED_ROLE: "unauthorized"
      # keycloak jwks endpoint
      HASURA_GRAPHQL_JWT_SECRET: ${HASURA_GRAPHQL_JWT_SECRET}
      HASURA_GRAPHQL_ADMIN_SECRET: ${KEYCLOAK_ADMIN_CLIENT_SECRET}
      ACTIONS_ADMIN_SECRET: ${KEYCLOAK_ADMIN_CLIENT_SECRET}
      HARVEST_DOMAIN: ${HARVEST_DOMAIN}
    depends_on:
      devcontainer:
        condition: service_started
      data-connector-agent:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      configure-minio:
        condition: service_completed_successfully

  data-connector-agent:
    profiles: ["full", "base"]
    container_name: data-connector-agent
    image: hasura/graphql-data-connector:v2.31.0
    restart: always
    ports:
      - 8081:8081
    environment:
      QUARKUS_LOG_LEVEL: ERROR # FATAL, ERROR, WARN, INFO, DEBUG, TRACE
      ## https://quarkus.io/guides/opentelemetry#configuration-reference
      QUARKUS_OPENTELEMETRY_ENABLED: "false"
      ## QUARKUS_OPENTELEMETRY_TRACER_EXPORTER_OTLP_ENDPOINT: http://jaeger:4317
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/api/v1/athena/health"]
      interval: 5s
      timeout: 10s
      retries: 25
      start_period: 5s

  postgres-keycloak:
    profiles: ["full", "base"]
    # Required by VACUUM ANALYZE to work properly
    shm_size: 256m
    image: sequentech.local/postgresql
    container_name: postgres-keycloak
    restart: unless-stopped
    volumes:
      - keycloak_db_data:/var/lib/postgresql
      - keycloak_db_logs:/logs
    command: postgres -c 'config_file=/etc/postgresql/postgresql.conf'
    environment:
      POSTGRES_PASSWORD: ${KC_DB_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready  -U postgres"]
      interval: 5s
      timeout: 10s
      retries: 15
      start_period: 5s
    # depending on the `postgres` container to ensure that the
    # `sequentech.local/postgresql` image is locally build, so that we can reuse
    # it:
    depends_on:
      - postgres

  keycloak:
    profiles: ["full", "base"]
    container_name: keycloak
    build:
      context: ../packages/
      dockerfile: Dockerfile.keycloak
    restart: always
    ports:
      - 8090:8090
    #recommend way for docker-outside-of-docker is using devcontainer.json forwardPorts
    #More info: https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-from-docker-compose/.devcontainer/docker-compose.yml#L28
    environment:
      AMQP_ADDR: ${AMQP_ADDR}
      KC_HOSTNAME: ${KC_HOSTNAME}
      KC_HOSTNAME_STRICT: ${KC_HOSTNAME_STRICT}
      KC_HTTP_PORT: ${KC_HTTP_PORT}
      KC_DB: ${KC_DB}
      KC_LOG_LEVEL: ${LOG_LEVEL}
      KC_DB_USERNAME: ${KC_DB_USERNAME}
      KC_DB_PASSWORD: ${KC_DB_PASSWORD}
      KC_DB_SCHEMA: ${KC_DB_SCHEMA}
      KC_DB_URL_HOST: ${KC_DB_URL_HOST}
      KC_DB_URL_PORT: ${KC_DB_URL_PORT}
      KC_DB_URL_DATABASE: ${KC_DB_URL_DATABASE}
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
      KEYCLOAK_URL: ${KEYCLOAK_URL}
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID}
      SUPER_ADMIN_TENANT_ID: ${SUPER_ADMIN_TENANT_ID}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      APP_VERSION: ${APP_VERSION}
      APP_HASH: ${APP_HASH}
      KC_OTP_RESEND_INTERVAL: ${KC_OTP_RESEND_INTERVAL}
      HARVEST_DOMAIN: ${HARVEST_DOMAIN}
      HASURA_ENDPOINT: ${HASURA_ENDPOINT}
      TWILIO_ACCOUNT_SID: ${TWILIO_ACCOUNT_SID}
      TWILIO_AUTH_TOKEN: ${TWILIO_AUTH_TOKEN}
      TWILIO_SERVICE_SID: ${TWILIO_SERVICE_SID}
      ENV_SLUG: ${ENV_SLUG}
    #volumes:
      # https://www.keycloak.org/server/containers#_importing_a_realm_on_startup
      #- ./keycloak/import:/opt/keycloak/data/import:z
    # Below we're using the dummy email email sender provider but that's just for
    # development, in production we should still use the default SMTP email provider
    #
    # We are doing the same to configure the dummy sms sender provider
    #
    # More info here: https://www.keycloak.org/server/configuration-provider
    entrypoint: >
      /opt/keycloak/bin/kc.sh start-dev
      --features=preview
      --health-enabled=true
      --spi-user-profile-declarative-user-profile-read-only-attributes=area-id,tenant-id
      --spi-user-profile-declarative-user-profile-admin-read-only-attributes=sequent.admin-read-only.*
      -Dkeycloak.profile.feature.upload_scripts=enabled
      --spi-email-sender-provider=dummy
      --spi-email-sender-dummy-enabled=true
      --spi-email-sender-default-enabled=false
      --spi-sms-sender-provider=dummy
      --spi-sms-sender-dummy-enabled=true
      --spi-sms-sender-aws-enabled=false
      --import-realm
    depends_on:
      postgres-keycloak:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://127.0.0.1:8090/health/live"]
      interval: 5s
      timeout: 10s
      retries: 25
      start_period: 5s

  # admin-portal:
  #   profiles: ["full"]
  #   image: sequentech.local/frontend
  #   container_name: admin-portal
  #   build:
  #     context: ../packages/
  #     dockerfile: Dockerfile
  #   #restart: always
  #   #recommend way for docker-outside-of-docker is using devcontainer.json forwardPorts
  #   #More info: https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-from-docker-compose/.devcontainer/docker-compose.yml#L28
  #   ports:
  #     - 3002:3000
  #   stdin_open: true
  #   entrypoint: ["/workspaces/step/packages/init.sh", "start:admin-portal"]
  #   working_dir: /workspaces/step/packages
  #   volumes_from:
  #     - devcontainer
  #   environment:
  #       MAX_DIFF_LINES: ${MAX_DIFF_LINES}
  #       SECONDS_TO_SHOW_COUNTDOWN: ${SECONDS_TO_SHOW_COUNTDOWN}
  #       SECONDS_TO_SHOW_ALRET: ${SECONDS_TO_SHOW_ALRET}

  # voting-portal:
  #   profiles: ["full"]
  #   image: sequentech.local/frontend
  #   container_name: voting-portal
  #   # depending on the `admin-portal` container to ensure that the
  #   # `sequentech.local/frontend` image is locally build, so that we can reuse
  #   # it:
  #   depends_on:
  #     - admin-portal
  #   #restart: always
  #   #recommend way for docker-outside-of-docker is using devcontainer.json forwardPorts
  #   #More info: https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-from-docker-compose/.devcontainer/docker-compose.yml#L28
  #   ports:
  #     - 3000:3000
  #   stdin_open: true
  #   entrypoint: ["./init.sh", "start"]
  #   volumes:
  #      - ${LOCAL_WORKSPACE_FOLDER}/packages:/usr/src/app

  immudb:
    profiles: ["full", "base"]
    container_name: immudb
    image: sequentech.local/immudb
    build:
      context: ../packages/
      dockerfile: ./Dockerfile.immudb
    restart: always
    environment:
      - IMMUDB_PGSQL_SERVER=true
    #recommend way for docker-outside-of-docker is using devcontainer.json forwardPorts
    #More info: https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-from-docker-compose/.devcontainer/docker-compose.yml#L28
    ports:
      - 3322:3322 # immudb service
      - 3324:9497 # prometheus
      - 3325:8080 # web console
    volumes:
      - immudb_data:/var/lib/immudb
      - immudb_logs:/var/log/immudb
    depends_on:
      - immudb-init

  mock_server:
    profiles: ["full"]
    stdin_open: true
    image: sequentech.local/cargo-packages
    container_name: mock_server
    volumes_from:
      - devcontainer
    depends_on:
      - devcontainer
    working_dir: /workspaces/step/packages/e2e/src/mock_server
    entrypoint: [
      "cargo",
      "watch",
      "--why",
      # ignoring rust-local-target/ since it's used for local builds and
      # otherwise triggers a rebuild all the time (probably rust analyzer)
      "--ignore",
      "rust-local-target/",
      "-x",
      "run"
    ]
    environment:
      ROCKET_ADDRESS: ${ROCKET_ADDRESS}
      ROCKET_PORT: ${MOCK_SERVER_PORT}
      MOCK_SERVER_PORT: ${MOCK_SERVER_PORT}
    ports:
     - ${MOCK_SERVER_PORT}:${MOCK_SERVER_PORT}

  janitor:
    profiles: ["full"]
    container_name: janitor
    build:
      context: ../packages/windmill/external-bin/janitor/scripts
      dockerfile: Dockerfile.janitor
    image: sequentech.local/janitor
    stdin_open: true
    tty: true
    working_dir: /opt/sequent-step
    entrypoint: ["/bin/bash"]
    environment:
      - PYTHONUNBUFFERED=1
  
  harvest:
    profiles: ["full", "base"]
    stdin_open: true
    image: sequentech.local/cargo-packages
    container_name: harvest
    build:
      context: ../packages/
      dockerfile: ./Dockerfile.cargo-packages
    volumes_from:
      - devcontainer
    depends_on:
      - devcontainer
    working_dir: /workspaces/step/packages/harvest
    entrypoint: ["/usr/local/bin/entrypoint.sh"]
    command: [
      "cargo",
      "watch",
      "--why",
      # ignoring rust-local-target/ since it's used for local builds and
      # otherwise triggers a rebuild all the time (probably rust analyzer)
      "--ignore",
      "rust-local-target/",
      "-x",
      "run"
    ]
    environment:
      RUST_BACKTRACE: ${RUST_BACKTRACE}
      SUPER_ADMIN_TENANT_ID: ${SUPER_ADMIN_TENANT_ID}
      LOG_LEVEL: ${LOG_LEVEL}
      ROCKET_ADDRESS: ${ROCKET_ADDRESS}
      ROCKET_PORT: ${HARVEST_PORT}
      RUSTFLAGS: ${RUSTFLAGS}
      CARGO_TERM_COLOR: ${CARGO_TERM_COLOR}
      IMMUDB_USER: ${IMMUDB_USER}
      IMMUDB_PASSWORD: ${IMMUDB_PASSWORD}
      IMMUDB_SERVER_URL: ${IMMUDB_SERVER_URL}
      SMS_TRANSPORT_NAME: ${SMS_TRANSPORT_NAME}
      AWS_SNS_ATTRIBUTES: ${AWS_SNS_ATTRIBUTES}
      EMAIL_TRANSPORT_NAME: ${EMAIL_TRANSPORT_NAME}
      EMAIL_FROM: ${EMAIL_FROM}
      KEYCLOAK_DB__USER: ${KEYCLOAK_DB__USER}
      KEYCLOAK_DB__PASSWORD: ${KEYCLOAK_DB__PASSWORD}
      KEYCLOAK_DB__HOST: ${KEYCLOAK_DB__HOST}
      KEYCLOAK_DB__PORT: ${KEYCLOAK_DB__PORT}
      KEYCLOAK_DB__DBNAME: ${KEYCLOAK_DB__DBNAME}
      KEYCLOAK_DB__MANAGER__RECYCLING_METHOD: ${KEYCLOAK_DB__MANAGER__RECYCLING_METHOD}
      AMQP_ADDR: ${AMQP_ADDR}
      KEYCLOAK_URL: ${KEYCLOAK_URL}
      KEYCLOAK_ELECTION_EVENT_REALM_CONFIG_PATH: ${KEYCLOAK_ELECTION_EVENT_REALM_CONFIG_PATH}
      KEYCLOAK_TENANT_REALM_CONFIG_PATH: ${KEYCLOAK_TENANT_REALM_CONFIG_PATH}
      KEYCLOAK_ADMIN_CLIENT_ID: ${KEYCLOAK_ADMIN_CLIENT_ID}
      KEYCLOAK_ADMIN_CLIENT_SECRET: ${KEYCLOAK_ADMIN_CLIENT_SECRET}
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      HASURA_ENDPOINT: ${HASURA_ENDPOINT}
      HASURA_DB__USER: ${HASURA_DB__USER}
      HASURA_DB__PASSWORD: ${HASURA_DB__PASSWORD}
      HASURA_DB__HOST: ${HASURA_DB__HOST}
      HASURA_DB__PORT: ${HASURA_DB__PORT}
      HASURA_DB__DBNAME: ${HASURA_DB__DBNAME}
      HASURA_DB__MANAGER__RECYCLING_METHOD: ${HASURA_DB__MANAGER__RECYCLING_METHOD}
      VOTING_PORTAL_URL: ${VOTING_PORTAL_URL}
      KEYCLOAK_PUBLIC_URL: ${KEYCLOAK_PUBLIC_URL}
      LOW_SQL_LIMIT: ${LOW_SQL_LIMIT}
      DEFAULT_SQL_LIMIT: ${DEFAULT_SQL_LIMIT}
      DEFAULT_SQL_BATCH_SIZE: ${DEFAULT_SQL_BATCH_SIZE}
      HARVEST_PROBE_ADDR: ${HARVEST_PROBE_ADDR}
      HARVEST_PROBE_LIVE_PATH: ${HARVEST_PROBE_LIVE_PATH}
      HARVEST_PROBE_READY_PATH: ${HARVEST_PROBE_READY_PATH}
      KEYCLOAK_VOTER_GROUP_NAME: ${KEYCLOAK_VOTER_GROUP_NAME}
      AWS_S3_PRIVATE_URI: ${AWS_S3_PRIVATE_URI}
      AWS_S3_PUBLIC_URI: ${AWS_S3_PUBLIC_URI}
      AWS_S3_BUCKET: ${AWS_S3_BUCKET}
      AWS_S3_PUBLIC_BUCKET: ${AWS_S3_PUBLIC_BUCKET}
      AWS_S3_ACCESS_KEY: ${AWS_S3_ACCESS_KEY}
      AWS_S3_ACCESS_SECRET: ${AWS_S3_ACCESS_SECRET}
      PUBLIC_ASSETS_PATH: ${PUBLIC_ASSETS_PATH}

      # used by AWS S3 to "load_from_env()". Don't use this in production, not
      # needed. Instead, in production we'll use Web Identity Tokens
      AWS_ACCESS_KEY_ID: ${AWS_S3_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${AWS_S3_ACCESS_SECRET}

      SECRETS_BACKEND: ${SECRETS_BACKEND}
      MASTER_SECRET: ${MASTER_SECRET}
      AWS_SM_KEY_PREFIX: ${AWS_SM_KEY_PREFIX}

      AWS_S3_UPLOAD_EXPIRATION_SECS: ${AWS_S3_UPLOAD_EXPIRATION_SECS}
      AWS_S3_FETCH_EXPIRATION_SECS: ${AWS_S3_FETCH_EXPIRATION_SECS}
      AWS_REGION: ${AWS_REGION}
      AWS_S3_MAX_UPLOAD_BYTES: ${AWS_S3_MAX_UPLOAD_BYTES}

      CLOUDFLARE_ZONE: ${CLOUDFLARE_ZONE}
      CLOUDFLARE_API_KEY: ${CLOUDFLARE_API_KEY}
      CUSTOM_URLS_IP_DNS_CONTENT: ${CUSTOM_URLS_IP_DNS_CONTENT}
      B3_PG_HOST: ${B3_PG_HOST}
      B3_PG_PORT: ${B3_PG_PORT}
      B3_PG_USER: ${B3_PG_USER}
      B3_PG_PASSWORD: ${B3_PG_PASSWORD}
      B3_PG_DATABASE: ${B3_PG_DATABASE}
      DOC_RENDERER_BACKEND: ${DOC_RENDERER_BACKEND}
      ENV_SLUG: ${ENV_SLUG}

    #recommend way for docker-outside-of-docker is using devcontainer.json forwardPorts
    #More info: https://github.com/microsoft/vscode-dev-containers/blob/main/containers/docker-from-docker-compose/.devcontainer/docker-compose.yml#L28
    ports:
     - ${HARVEST_PORT}:${HARVEST_PORT}

  trustee1:
    profiles: ["full"]
    stdin_open: true
    container_name: trustee1
    image: sequentech.local/braid
    volumes:
      - trustee1_data:/opt/braid
    # TODO: be able to reuse the `sequentech.local/cargo-packages` image
    build:
      context: ../packages/
      dockerfile: ./braid/Dockerfile
    environment:
        TRUSTEE_NAME: trustee1
        TRUSTEE_CONFIG_PATH: ${TRUSTEE1_CONFIG}
        IGNORE_BOARDS: ${IGNORE_BOARDS}
        SECRETS_BACKEND: ${SECRETS_BACKEND}
        VAULT_SERVER_URL: ${VAULT_SERVER_URL}
        VAULT_TOKEN: ${VAULT_TOKEN}
        B3_URL: ${B3_URL}
    depends_on:
      - immudb
      - b3

  trustee2:
    profiles: ["full"]
    stdin_open: true
    image: sequentech.local/braid
    container_name: trustee2
    volumes:
      - trustee2_data:/opt/braid
    # depending on the `trustee1` container to ensure that the
    # `sequentech.local/braid` image is locally build, so that we can reuse
    # it:
    depends_on:
      - trustee1
      - immudb
    environment:
        TRUSTEE_NAME: trustee2
        TRUSTEE_CONFIG_PATH: ${TRUSTEE2_CONFIG}
        IGNORE_BOARDS: ${IGNORE_BOARDS}
        SECRETS_BACKEND: ${SECRETS_BACKEND}
        VAULT_SERVER_URL: ${VAULT_SERVER_URL}
        VAULT_TOKEN: ${VAULT_TOKEN}
        B3_URL: ${B3_URL}

  # trustee3:
  #   profiles: ["full"]
  #   stdin_open: true
  #   image: sequentech.local/braid
  #   container_name: trustee3
  #   volumes:
  #     - trustee3_data:/opt/braid
  #   # depending on the `trustee1` container to ensure that the
  #   # `sequentech.local/braid` image is locally build, so that we can reuse
  #   # it:
  #   depends_on:
  #     - trustee1
  #     - immudb
  #   environment:
  #       TRUSTEE_NAME: trustee3
  #       TRUSTEE_CONFIG_PATH: ${TRUSTEE3_CONFIG}
  #       IGNORE_BOARDS: ${IGNORE_BOARDS}
  #       SECRETS_BACKEND: ${SECRETS_BACKEND}
  #       VAULT_SERVER_URL: ${VAULT_SERVER_URL}
  #       VAULT_TOKEN: ${VAULT_TOKEN}
  #       B3_URL: ${B3_URL}

  # Create collection in immudb
  immudb-log-audit-init:
    profiles: ["full", "base"]
    image: sequentech.local/immudb-log-audit
    container_name: immudb-log-audit-init
    build:
      context: ../vendor/immudb-log-audit
      dockerfile: Dockerfile.immudb
    command: >
      create sql pgaudit_hasura
      --parser pgauditjsonlog
      --log-level debug
      --immudb-user ${IMMUDB_USER}
      --immudb-password ${IMMUDB_PASSWORD}
      --immudb-host ${IMMUDB_HOST}
      --immudb-port ${IMMUDB_PORT}
      --immudb-database ${IMMUDB_LOGS_DB}
    depends_on:
      - postgres
      - immudb

  # Send audit logs to immudb
  immudb-log-audit:
    profiles: ["full", "base"]
    image: sequentech.local/immudb-log-audit
    container_name: immudb-log-audit
    command: >
      tail file pgaudit_hasura "/logs/*.json"
      --follow
      --parser pgauditjsonlog
      --file-registry-dir=/data
      --log-level debug
      --immudb-user ${IMMUDB_USER}
      --immudb-password ${IMMUDB_PASSWORD}
      --immudb-host ${IMMUDB_HOST}
      --immudb-port ${IMMUDB_PORT}
      --immudb-database ${IMMUDB_LOGS_DB}
    volumes:
      - 'immudb_log_audit_data:/data'
      - 'db_logs:/logs'
    # make sure to depend on the `immudb-log-audit-init` container to ensure
    # that the `sequentech.local/immudb-log-audit` image is locally build, so
    # that we can reuse it:
    depends_on:
      - postgres
      - immudb
      - immudb-log-audit-init
    environment:
      IMMUDB_LOG_AUDIT_PROBE_ADDR: ${IMMUDB_LOG_AUDIT_PROBE_ADDR}
      IMMUDB_LOG_AUDIT_PROBE_LIVE_PATH: ${IMMUDB_LOG_AUDIT_PROBE_LIVE_PATH}

  # Create collection in immudb
  immudb-log-audit-init-keycloak:
    profiles: ["full", "base"]
    image: sequentech.local/immudb-log-audit
    container_name: immudb-log-audit-init-keycloak
    command: >
      create sql pgaudit_keycloak
      --parser pgauditjsonlog
      --log-level debug
      --immudb-user ${IMMUDB_USER}
      --immudb-password ${IMMUDB_PASSWORD}
      --immudb-host ${IMMUDB_HOST}
      --immudb-port ${IMMUDB_PORT}
      --immudb-database ${IMMUDB_LOGS_DB}
    depends_on:
      - postgres
      - immudb
      - immudb-log-audit-init

  # Send audit logs to immudb
  immudb-log-audit-keycloak:
    profiles: ["full", "base"]
    image: sequentech.local/immudb-log-audit
    container_name: immudb-log-audit-keycloak
    command: >
      tail file pgaudit_keycloak "/logs/*.json"
      --follow
      --parser pgauditjsonlog
      --file-registry-dir=/data
      --log-level debug
      --immudb-user ${IMMUDB_USER}
      --immudb-password ${IMMUDB_PASSWORD}
      --immudb-host ${IMMUDB_HOST}
      --immudb-port ${IMMUDB_PORT}
      --immudb-database ${IMMUDB_LOGS_DB}
    volumes:
      - 'keycloak_immudb_log_audit_data:/data'
      - 'keycloak_db_logs:/logs'
    # make sure to depend on the `immudb-log-audit-init` container to ensure
    # that the `sequentech.local/immudb-log-audit` image is locally build, so
    # that we can reuse it:
    depends_on:
      - postgres
      - immudb
      - immudb-log-audit-init-keycloak

  rabbitmq:
    profiles: ["full", "base"]
    image: rabbitmq:3.12.11-management
    container_name: rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_DEFAULT_USER}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_DEFAULT_PASS}
    ports:
      - "5672:5672"
      - "15672:15672"

  immudb-init:
    profiles: ["full", "base"]
    stdin_open: true
    image: sequentech.local/cargo-packages
    container_name: immudb-init
    # make sure to depend on the `harvest` container to ensure
    # that the `sequentech.local/cargo-packages` image is locally build, so
    # that we can reuse it:
    depends_on:
      - harvest
    volumes:
      - ./keycloak/import:/import:z
      - ../packages/:/app:z
    restart: always
    working_dir: /app/immu-board
    entrypoint: [
      "cargo",
      "run",
      "--bin",
      "bb_helper",
      "--",
      "--server-url",
      "${IMMUDB_SERVER_URL}",
      "--username",
      "${IMMUDB_USER}",
      "--password",
      "${IMMUDB_PASSWORD}",
      "--board-dbname",
      "${IMMUDB_BOARD_DB_NAME}",
      "--cache-dir",
      "/tmp/immu-board",
      "upsert-board-db"
    ]
    env_file:
      - .env

  registry:
    profiles: ["full", "lambdas"]
    container_name: registry
    image: registry:2
    network_mode: host

  openwhisk:
    profiles: ["full", "lambdas"]
    container_name: openwhisk
    image: openwhisk/standalone:nightly
    volumes:
    - type: bind
      source: /var/run/docker.sock
      target: /var/run/docker.sock
    network_mode: host

  doc_renderer_lambda_build:
    profiles: ["full", "lambdas"]
    container_name: doc_renderer_lambda_build
    image: ${REGISTRY}/doc_renderer:latest
    build:
      context: ./../packages
      dockerfile: ./../packages/orare/doc_renderer/Dockerfile
      network: host
      args:
        - BASE_IMAGE=alpine:3.17@sha256:8fc3dacfb6d69da8d44e42390de777e48577085db99aa4e4af35f483eb08b989
        - FEATURES=openwhisk
    # This is not meant to be a running container, but rather a
    # trigger to build the lambda to a container image that will later
    # on be pulled by OpenWhisk
    entrypoint: ["true"]
    env_file:
      - .env

  doc_renderer_lambda_publish_locally:
    profiles: ["full", "lambdas"]
    image: docker:latest
    command: |
      push ${REGISTRY}/doc_renderer:latest
    depends_on:
      - registry
      - doc_renderer_lambda_build
    volumes:
    - type: bind
      source: /var/run/docker.sock
      target: /var/run/docker.sock

  doc_renderer:
    profiles: ["full", "lambdas"]
    container_name: doc_renderer
    image: openwhisk/standalone:nightly
    network_mode: host
    entrypoint: /bin/deploy-doc_renderer.sh
    volumes:
      - ./openwhisk/deploy-doc_renderer.sh:/bin/deploy-doc_renderer.sh
    volumes_from:
      - devcontainer
    depends_on:
      - openwhisk
      - registry
      - doc_renderer_lambda_publish_locally
    env_file:
      - .env

  windmill:
    profiles: ["full", "base"]
    stdin_open: true
    image: sequentech.local/cargo-packages
    container_name: windmill
    # make sure to depend on the `harvest` container to ensure
    # that the `sequentech.local/cargo-packages` image is locally build, so
    # that we can reuse it:
    restart: always
    working_dir: /workspaces/step/packages/windmill
    entrypoint: ["/usr/local/bin/entrypoint.sh"]
    command: [
      "cargo",
      "watch",
      "--why",
      # ignoring rust-local-target/ since it's used for local builds and
      # otherwise triggers a rebuild all the time (probably rust analyzer)
      "--ignore",
      "rust-local-target/",
      "--ignore",
      "windmill/core",
      "-x",
      "run --bin main consume -q beat short_queue tally_queue reports_queue communication_queue import_export_queue electoral_log_batch_queue electoral_log_beat_queue --prefetch-count 1"
    ]
    volumes_from:
      - devcontainer
    depends_on:
      - devcontainer
      #- postgres
      #- postgres-keycloak
    environment:
      RUSTFLAGS: ${RUSTFLAGS}
      RUST_BACKTRACE: ${RUST_BACKTRACE}
      LOG_LEVEL: ${LOG_LEVEL}
      CARGO_TERM_COLOR: ${CARGO_TERM_COLOR}
      HASURA_ENDPOINT: ${HASURA_ENDPOINT}
      AMQP_ADDR: ${AMQP_ADDR}
      KEYCLOAK_URL: ${KEYCLOAK_URL}
      IMMUDB_SERVER_URL: ${IMMUDB_SERVER_URL}
      IMMUDB_USER: ${IMMUDB_USER}
      IMMUDB_PASSWORD: ${IMMUDB_PASSWORD}
      IMMUDB_INDEX_DB: ${IMMUDB_INDEX_DB}
      AWS_S3_PRIVATE_URI: ${AWS_S3_PRIVATE_URI}
      AWS_S3_PUBLIC_URI: ${AWS_S3_PUBLIC_URI}
      AWS_S3_BUCKET: ${AWS_S3_BUCKET}
      AWS_S3_PUBLIC_BUCKET: ${AWS_S3_PUBLIC_BUCKET}
      AWS_S3_ACCESS_KEY: ${AWS_S3_ACCESS_KEY}
      AWS_S3_ACCESS_SECRET: ${AWS_S3_ACCESS_SECRET}
      AWS_S3_JWKS_CACHE_POLICY: ${AWS_S3_JWKS_CACHE_POLICY}

      # used by AWS S3 to "load_from_env()". Don't use this in production, not
      # needed. Instead, in production we'll use Web Identity Tokens
      AWS_ACCESS_KEY_ID: ${AWS_S3_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${AWS_S3_ACCESS_SECRET}

      AWS_S3_UPLOAD_EXPIRATION_SECS: ${AWS_S3_UPLOAD_EXPIRATION_SECS}
      AWS_S3_FETCH_EXPIRATION_SECS: ${AWS_S3_FETCH_EXPIRATION_SECS}
      AWS_REGION: ${AWS_REGION}
      AWS_S3_MAX_UPLOAD_BYTES: ${AWS_S3_MAX_UPLOAD_BYTES}

      SECRETS_BACKEND: ${SECRETS_BACKEND}
      MASTER_SECRET: ${MASTER_SECRET}
      AWS_SM_KEY_PREFIX: ${AWS_SM_KEY_PREFIX}

      KEYCLOAK_ELECTION_EVENT_REALM_CONFIG_PATH: ${KEYCLOAK_ELECTION_EVENT_REALM_CONFIG_PATH}
      KEYCLOAK_TENANT_REALM_CONFIG_PATH: ${KEYCLOAK_TENANT_REALM_CONFIG_PATH}
      KEYCLOAK_ADMIN_CLIENT_ID: ${KEYCLOAK_ADMIN_CLIENT_ID}
      KEYCLOAK_ADMIN_CLIENT_SECRET: ${KEYCLOAK_ADMIN_CLIENT_SECRET}
      KEYCLOAK_CLIENT_ID: ${KEYCLOAK_CLIENT_ID}
      KEYCLOAK_CLIENT_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      SUPER_ADMIN_TENANT_ID: ${SUPER_ADMIN_TENANT_ID}
      VOTING_PORTAL_URL: ${VOTING_PORTAL_URL}
      KEYCLOAK_PUBLIC_URL: ${KEYCLOAK_PUBLIC_URL}
      SMS_TRANSPORT_NAME: ${SMS_TRANSPORT_NAME}
      AWS_SNS_ATTRIBUTES: ${AWS_SNS_ATTRIBUTES}
      EMAIL_TRANSPORT_NAME: ${EMAIL_TRANSPORT_NAME}
      EMAIL_FROM: ${EMAIL_FROM}
      KEYCLOAK_DB__USER: ${KEYCLOAK_DB__USER}
      KEYCLOAK_DB__PASSWORD: ${KEYCLOAK_DB__PASSWORD}
      KEYCLOAK_DB__HOST: ${KEYCLOAK_DB__HOST}
      KEYCLOAK_DB__PORT: ${KEYCLOAK_DB__PORT}
      KEYCLOAK_DB__DBNAME: ${KEYCLOAK_DB__DBNAME}
      KEYCLOAK_DB__MANAGER__RECYCLING_METHOD: ${KEYCLOAK_DB__MANAGER__RECYCLING_METHOD}
      HASURA_DB__USER: ${HASURA_DB__USER}
      HASURA_DB__PASSWORD: ${HASURA_DB__PASSWORD}
      HASURA_DB__HOST: ${HASURA_DB__HOST}
      HASURA_DB__PORT: ${HASURA_DB__PORT}
      HASURA_DB__DBNAME: ${HASURA_DB__DBNAME}
      HASURA_DB__MANAGER__RECYCLING_METHOD: ${HASURA_DB__MANAGER__RECYCLING_METHOD}
      LOW_SQL_LIMIT: ${LOW_SQL_LIMIT}
      DEFAULT_SQL_LIMIT: ${DEFAULT_SQL_LIMIT}
      DEFAULT_SQL_BATCH_SIZE: ${DEFAULT_SQL_BATCH_SIZE}
      WINDMILL_PROBE_ADDR: ${WINDMILL_PROBE_ADDR}
      WINDMILL_PROBE_LIVE_PATH: ${WINDMILL_PROBE_LIVE_PATH}
      WINDMILL_PROBE_READY_PATH: ${WINDMILL_PROBE_READY_PATH}
      APP_VERSION: ${APP_VERSION}
      APP_HASH: ${APP_HASH}

      PUBLIC_ASSETS_PATH: ${PUBLIC_ASSETS_PATH}
       #Demo key
      DEMO_PUBLIC_KEY: ${DEMO_PUBLIC_KEY}
      B3_PG_HOST: ${B3_PG_HOST}
      B3_PG_PORT: ${B3_PG_PORT}
      B3_PG_USER: ${B3_PG_USER}
      B3_PG_PASSWORD: ${B3_PG_PASSWORD}
      B3_PG_DATABASE: ${B3_PG_DATABASE}
      DOC_RENDERER_BACKEND: ${DOC_RENDERER_BACKEND}
      BALLOT_VERIFIER_URL: ${BALLOT_VERIFIER_URL}
      ENV_SLUG: ${ENV_SLUG}

  beat:
    profiles: ["full", "base"]
    stdin_open: true
    image: sequentech.local/cargo-packages
    container_name: beat
    volumes_from:
      - devcontainer
    # make sure to depend on the `harvest` container to ensure
    # that the `sequentech.local/cargo-packages` image is locally build, so
    # that we can reuse it:
    depends_on:
      - devcontainer
      - harvest
    working_dir: /workspaces/step/packages/windmill
    entrypoint: ["/usr/local/bin/entrypoint.sh"]
    command: [
      "cargo",
      "watch",
      "--why",
      # ignoring rust-local-target/ since it's used for local builds and
      # otherwise triggers a rebuild all the time (probably rust analyzer)
      "--ignore",
      "rust-local-target/",
      "-x",
      "run --bin beat"
    ]
    environment:
      RUSTFLAGS: ${RUSTFLAGS}
      LOG_LEVEL: ${LOG_LEVEL}
      CARGO_TERM_COLOR: ${CARGO_TERM_COLOR}
      HASURA_ENDPOINT: ${HASURA_ENDPOINT}
      AMQP_ADDR: ${AMQP_ADDR}
      BEAT_PROBE_ADDR: ${BEAT_PROBE_ADDR}
      BEAT_PROBE_LIVE_PATH: ${BEAT_PROBE_LIVE_PATH}
      BEAT_PROBE_READY_PATH: ${BEAT_PROBE_READY_PATH}
      ENV_SLUG: ${ENV_SLUG}

  postgres-b3:
    profiles: ["full", "base"]
    image: sequentech.local/postgresql-b3
    container_name: postgres-b3
    build:
      context: ./postgresql-b3
      dockerfile: Dockerfile
    restart: unless-stopped
    volumes:
      - b3_db_data:/var/lib/postgresql
      - b3_db_logs:/logs
    command: postgres -c 'config_file=/etc/postgresql/postgresql.conf'
    environment:
      POSTGRES_PASSWORD: ${B3_PG_PASSWORD}
    user: "999:999"  # This ensures that the postgres user inside the container is used
    healthcheck:
      test: ["CMD-SHELL", "pg_isready  -U postgres"]
      interval: 5s
      timeout: 10s
      retries: 15
      start_period: 5s
    depends_on:
      - postgres-volume-init

  b3:
    profiles: ["full", "base"]
    stdin_open: true
    image: sequentech.local/cargo-packages
    container_name: b3
    restart: always
    ports:
      - "50051:50051"
    build:
      context: ../packages/
      dockerfile: ./Dockerfile.cargo-packages
    volumes_from:
      - devcontainer
    depends_on:
      #- devcontainer
      - postgres-b3
    working_dir: /workspaces/step/packages/b3
    entrypoint: [
      "cargo",
      "watch",
      "--why",
      # ignoring rust-local-target/ since it's used for local builds and
      # otherwise triggers a rebuild all the time (probably rust analyzer)
      "--ignore",
      "rust-local-target/",
      "--features",
      "\"server sqlcopy\"",
      "-x",
      "run --bin server -- --host $B3_PG_HOST --port $B3_PG_PORT --username $B3_PG_USER --password $B3_PG_PASSWORD --database $B3_PG_DATABASE --bind $B3_BIND",
    ]
    environment:
      RUSTFLAGS: ${RUSTFLAGS}
      RUST_BACKTRACE: ${RUST_BACKTRACE}
      LOG_LEVEL: ${LOG_LEVEL}
      CARGO_TERM_COLOR: ${CARGO_TERM_COLOR}
      B3_PG_HOST: ${B3_PG_HOST}
      B3_PG_PORT: ${B3_PG_PORT}
      B3_PG_USER: ${B3_PG_USER}
      B3_PG_PASSWORD: ${B3_PG_PASSWORD}
      B3_PG_DATABASE: ${B3_PG_DATABASE}
      B3_BIND: ${B3_BIND}

volumes:
  db_data:
  db_logs:
  keycloak_db_logs:
  immudb_log_audit_data:
  keycloak_immudb_log_audit_data:
  keycloak_db_data:
  immudb_data:
  immudb_logs:
  minio_storage:
  trustee1_data:
  trustee2_data:
  trustee3_data:
  protocol_manager_data:
  b3_db_logs:
  b3_db_data:
